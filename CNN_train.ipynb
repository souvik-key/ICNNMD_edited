{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os ## added\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' ## added\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "import functools\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '/gpu:0'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lime model for explaination\n",
    "\n",
    "model == cnn model\n",
    "X == pixel map\n",
    "y ==  labels correspond to each map\n",
    "m == at what stride the resampling frames will be chosen for model explaination\n",
    "\n",
    "### for more detail on the lime protocol: https://towardsdatascience.com/how-to-explain-image-classifiers-using-lime-e364097335b4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,sys\n",
    "try:\n",
    "    import lime\n",
    "except:\n",
    "    sys.path.append(os.path.join('..', '..')) # add the current directory\n",
    "    import lime\n",
    "from lime.wrappers.scikit_image import SegmentationAlgorithm\n",
    "from lime import lime_image\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def cnn_lime(model, X, y, m): ## m = stride\n",
    "\n",
    "    explainer = lime_image.LimeImageExplainer(verbose = False)\n",
    "    segmenter = SegmentationAlgorithm('slic', n_segments=100, compactness=1, sigma=1)\n",
    "    # =============================================================================\n",
    "\n",
    "    # =============================================================================\n",
    "\n",
    "    n_features = 100\n",
    "    n_samples = 1000\n",
    "\n",
    "    importance_pic = []\n",
    "    size = len(X[0])\n",
    "#     n_lime = int(len(y)/10)\n",
    "    n_lime = int(len(y)/m)\n",
    "    for i in range(size):\n",
    "        line = []\n",
    "        for j in range(size):\n",
    "            line.append(0)\n",
    "        importance_pic.append(line)\n",
    "\n",
    "    for i in range(n_lime):\n",
    "        print(i)\n",
    "#             explanation = explainer.explain_instance(X[i*10].astype('float64'),\n",
    "        explanation = explainer.explain_instance(X[i*m].astype('float64'),\n",
    "                                                # classifier_fn = model[j].predict_proba,  \n",
    "                                                 classifier_fn = (model.predict), ## added\n",
    "                                                 top_labels=2, hide_color=0, num_samples=n_samples, segmentation_fn=segmenter)\n",
    "        temp, mask = explanation.get_image_and_mask(y[i*m], positive_only=True, num_features=n_features, hide_rest=False)\n",
    "\n",
    "        importance_pic = np.array(importance_pic) + np.array(mask)\n",
    "            \n",
    "#             if(sum(sum(mask)))!=0:\n",
    "#                 mask = [x/(sum(sum(mask))) for x in mask]\n",
    "#                 importance_pic = np.array(importance_pic) + np.array(mask)\n",
    "\n",
    "\n",
    "    # =============================================================================\n",
    "    importance_pic = importance_pic / n_lime ## added \n",
    "    return importance_pic, explainer, segmenter\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load traj and pdb (Trajectory will be read using mdanalysis. So technically dcd/xtc should also work)\n",
    "For trajectory load related modifications, check traj_utils.py (function: traj_pre, load_traj, read_traj)\n",
    "The CNN modelling function now only do a binary classification. But we can extend it as per our need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## List of systems\n",
    "file0_1 = \"../../crRNA_R1-3_notREClobes.nc\"\n",
    "file0_2 = \"../../crRNA_R1-3_notREClobes.pdb\"\n",
    "file1_1 = \"../../tgRNA_R1-3_notREClobes.nc\"\n",
    "file1_2 = \"../../tgRNA_R1-3_notREClobes.pdb\"\n",
    "stride = 5\n",
    "# if_prt = 1\n",
    "# if_save = 1\n",
    "# if_dtl = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from . import traj_utils\n",
    "except Exception:\n",
    "    import traj_utils\n",
    "_, y_all, X_all = traj_utils.traj_pre(file0_1, file1_1, file0_2, file1_2, stride)\n",
    "#_, X_all, y_all = traj_utils.traj_pre(file0_1, file1_1, file0_2, file1_2)\n",
    "print(\"Preprocess Done.\\n\")\n",
    "\n",
    "## y_all: labels [0/1]\n",
    "## X_all: pixel_maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"length of y_all:\",len(y_all))\n",
    "print(\"length of X_all:\",len(X_all))\n",
    "# print(y_all[27990:])\n",
    "# print(X_all[1].shape)\n",
    "# X_1 = X_all[1].reshape(1,X_all[1].shape[0],X_all[1].shape[1],3)\n",
    "# print(X_1.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the data to train/test set\n",
    "K: k-fold cross-validation\n",
    "\n",
    "group: Divide each traj into 'group' number of groups in time order. Each group then randomly divided into 5 folds. take 4 fold from each group to make the training set and 1 fold for the validation set\n",
    "\n",
    "cap: population in each group\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from . import split_data\n",
    "except Exception:\n",
    "    import split_data\n",
    "group = 10\n",
    "cap = int(len(X_all) / group)\n",
    "k = 5\n",
    "X_train, y_train, X_test, y_test = split_data.split_by_group(X_all, y_all, group, cap, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(X_train[3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorical data encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from . import data_utils\n",
    "except Exception:\n",
    "    import data_utils\n",
    "num_classes = 2\n",
    "X_train, y_train, X_test, y_test = data_utils.data_encode(X_train, y_train, X_test, y_test, k, num_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(y_train[0][:10])\n",
    "# print(y_train[4][:10])\n",
    "# if (y_train[1] == y_train[4]).all():\n",
    "#     print(\"fuck\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train CNN\n",
    "batch_size: 32/64/128 should be fine.. may vary with number of inputs\n",
    "\n",
    "epochs: not less than 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from . import cnn_model\n",
    "except Exception:\n",
    "    import cnn_model\n",
    "\n",
    "batch_size = 32\n",
    "epochs = 10\n",
    "model = cnn_model.cnn_build(X_train, k, num_classes)\n",
    "model, history = cnn_model.cnn_train(model, X_train, y_train, X_test, y_test, k, batch_size, epochs, if_dtl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Evaluate your model\n",
    "try:\n",
    "    from . import model_evaluate\n",
    "except Exception:\n",
    "    import model_evaluate\n",
    "\n",
    "model_evaluate.eva_cross_acc_all(model, X_train, X_test, y_train, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Write your model\n",
    "for t in range(k):\n",
    "        model[t].save('model' + str(t) + '.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print((model[1].predict(X_train[1])>0.5).astype(\"int32\"))\n",
    "# print(y_train[1][1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_train_pre  = (model.predict(X_1))\n",
    "# print(y_train_pre)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model explaination using LIME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "importance_pic1 = []\n",
    "size = len(X_all[0])\n",
    "for i in range(size):\n",
    "    line = []\n",
    "    for j in range(size):\n",
    "        line.append(0)\n",
    "    importance_pic1.append(line)\n",
    "\n",
    "for i in list(range(5)): ## how many models you have created by k-fold CV\n",
    "    model = load_model(f\"model{i}.h5\")\n",
    "    importance_pic, explainer, segmenter = cnn_lime(model, X_all, y_all, 100)\n",
    "    print(importance_pic1)\n",
    "    print(f\"model{i} done.....\")\n",
    "#     print(len(importance_pic1))\n",
    "    importance_pic1 = np.array(importance_pic1) + np.array(importance_pic)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write the atom score xlsx file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xlsxwriter\n",
    "\n",
    "workbook = xlsxwriter.Workbook('atom.xlsx')\n",
    "worksheet = workbook.add_worksheet()\n",
    "size = len(importance_pic)\n",
    "# n_lime = int(len(y)/m)\n",
    "for i in range(size):\n",
    "    for j in range(size):\n",
    "        worksheet.write(i * size + j, 0, i * size + j + 1)\n",
    "        worksheet.write(i * size + j, 1, importance_pic1[i][j])\n",
    "workbook.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read the xlsx file and map the scores from atom to residue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"../../crRNA_R1-3_notREClobes.pdb\", \"r\")\n",
    "\n",
    "data = f.readlines()\n",
    "f.close()\n",
    "p = len(data) - 1\n",
    "data.pop(p)\n",
    "data.pop(p - 1)\n",
    "data.pop(0)\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "bond = []\n",
    "for line in data:\n",
    "    bond_line = line.split(' ')\n",
    "    while '' in bond_line:\n",
    "        bond_line.remove('')\n",
    "    bond.append([int(bond_line[1]), int(bond_line[4])])\n",
    "df1 = pd.DataFrame(bond) # added\n",
    "\n",
    "\n",
    "### ----- Added to read xlsx --- ###\n",
    "import pandas as pd\n",
    "import openpyxl\n",
    "df2 = pd.read_excel(\n",
    "  \"atom.xlsx\", header=None,\n",
    "  engine='openpyxl',\n",
    ")\n",
    " ### ------------------------------------####\n",
    "df1[2] = df2[1] ## added\n",
    "df3 = df1.drop([0], axis=1) ## added\n",
    "df4 = df3.groupby(by=[1], group_keys=False).mean() ## added\n",
    "df5 = df4.sort_values(by=df4.columns[0], ascending=False) ## added\n",
    "df5.to_csv(r'res_score_old.dat', header=None,  sep=' ', mode='a')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
